{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d16341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_nlp as keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "path = './second_dataset/mbti_1.csv' \n",
    "#path =  './third_dataset/MBTI 500.csv'\n",
    "df = pd.read_csv(path)\n",
    "train_fraction = 0.80\n",
    "val_fraction = 0.10\n",
    "vocab_size = 20000\n",
    "seq_len = 256\n",
    "batch_size = 64\n",
    "vocab = \"vocab\"\n",
    "seed = 0\n",
    "preset_token = [\"__url__\", \"__sep__\", \"__tar__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8226375f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df):\n",
    "    posts = df[\"posts\"]\n",
    "    types = df[\"type\"]\n",
    "    \n",
    "    posts = posts.astype(str).str.replace(r'http[^\\s|]+', ' __url__ ', regex=True)\n",
    "    posts = posts.astype(str).str.replace(r'|||', ' __sep__ ', regex=False)\n",
    "    posts = posts.astype(str).str.replace(r'(?i)\\b[IE][NS][TF][JP]\\b', ' __tar__ ', regex=True) # i.e. INTP, ENFJ\n",
    "    posts = posts.astype(str).str.replace(r\"\\s+\", \" \", regex=True) # remove excess spaces\n",
    "    \n",
    "    types = types.astype(str).str.replace(r'I|N|T|J', \"1\", regex=True)\n",
    "    types = types.astype(str).str.replace(r'E|S|F|P', \"0\", regex=True)\n",
    "    \n",
    "    df[\"posts\"] = posts\n",
    "    df[\"type\"] = types\n",
    "    \n",
    "    df[[\"I_E\", \"N_S\", \"T_F\", \"J_P\"]] = (\n",
    "        df[\"type\"].apply(lambda s: pd.Series([int(c) for c in s]))\n",
    "    ).astype(\"int32\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728a40e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      type                                              posts  I_E  N_S  T_F  \\\n",
      "0     1101  ' __url__ __sep__ __url__ __sep__ __tar__ and ...    1    1    0   \n",
      "1     0110  'I'm finding the lack of me in these posts ver...    0    1    1   \n",
      "2     1110  'Good one _____ __url__ __sep__ Of course, to ...    1    1    1   \n",
      "3     1111  'Dear __tar__ , I enjoyed our conversation the...    1    1    1   \n",
      "4     0111  'You're fired. __sep__ That's another silly mi...    0    1    1   \n",
      "...    ...                                                ...  ...  ...  ...   \n",
      "8670  1000  ' __url__ __sep__ IxFP just because I always t...    1    0    0   \n",
      "8671  0100  'So...if this thread already exists someplace ...    0    1    0   \n",
      "8672  1110  'So many questions when i do these things. I w...    1    1    1   \n",
      "8673  1100  'I am very conflicted right now when it comes ...    1    1    0   \n",
      "8674  1100  'It has been too long since I have been on per...    1    1    0   \n",
      "\n",
      "      J_P  \n",
      "0       1  \n",
      "1       0  \n",
      "2       0  \n",
      "3       1  \n",
      "4       1  \n",
      "...   ...  \n",
      "8670    0  \n",
      "8671    0  \n",
      "8672    0  \n",
      "8673    0  \n",
      "8674    0  \n",
      "\n",
      "[8675 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "# train/test\n",
    "df = preprocess_df(df)\n",
    "print(df)\n",
    "n = len(df)\n",
    "n_train = int(train_fraction * n)\n",
    "n_val = int((val_fraction + train_fraction) * n)\n",
    "\n",
    "train_df = df[ : n_train]\n",
    "val_df = df[n_train : n_val]\n",
    "test_df = df[n_val : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1485e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_tf_ds(posts, targets, training=False):\n",
    "    posts = posts.astype(str).to_numpy()\n",
    "    targets = targets.to_numpy(dtype=\"float32\")\n",
    "    \n",
    "    ds = tf.data.Dataset.from_tensor_slices((posts, targets))\n",
    "    if training:\n",
    "        ds = ds.shuffle(buffer_size=len(train_df), seed=seed, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds\n",
    "\n",
    "train_ds = create_tf_ds(train_df[\"posts\"], train_df[[\"I_E\", \"N_S\", \"T_F\", \"J_P\"]], training=True)\n",
    "val_ds   = create_tf_ds(val_df[\"posts\"],   val_df[[\"I_E\", \"N_S\", \"T_F\", \"J_P\"]], training=False)\n",
    "test_ds  = create_tf_ds(test_df[\"posts\"],  test_df[[\"I_E\", \"N_S\", \"T_F\", \"J_P\"]], training=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9963825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved in: C:\\Users\\jeldr\\ML-Project\\second_dataset\\tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "from tokenizers.trainers import WordPieceTrainer\n",
    "from tokenizers.normalizers import BertNormalizer\n",
    "from tokenizers.pre_tokenizers import BertPreTokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "train_texts = train_df[\"posts\"].astype(str).tolist()\n",
    "\n",
    "\n",
    "tok = Tokenizer(WordPiece(unk_token=\"__unk__\"))\n",
    "tok.normalizer = BertNormalizer(lowercase=True)\n",
    "tok.pre_tokenizer = BertPreTokenizer()\n",
    "\n",
    "trainer = WordPieceTrainer(vocab_size=vocab_size, special_tokens=preset_token)\n",
    "tok.train_from_iterator(train_texts, trainer=trainer)\n",
    "\n",
    "tok.save(path + \"/../tokenizer.json\")\n",
    "print(\"saved at:\", Path(path + \"/../tokenizer.json\").resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
